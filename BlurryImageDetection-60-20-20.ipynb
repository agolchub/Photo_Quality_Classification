{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_image_dir(Images_Path):\n",
    "    \n",
    "    image_classes = sorted([dirname for dirname in os.listdir(Images_Path)\n",
    "                      if os.path.isdir(os.path.join(Images_Path, dirname)) and not dirname.startswith(\".\") and not dirname.startswith(\"mblur\")])\n",
    "    image_classes.append('mblur')\n",
    "    \n",
    "    print(image_classes)\n",
    "    \n",
    "    x = [] # images as arrays\n",
    "    y = [] # labels Infiltration or Not_infiltration\n",
    "    WIDTH = 256\n",
    "    HEIGHT = 256\n",
    "  \n",
    "    print(\"Adding Images: \",end=\"\")\n",
    "    i = 0\n",
    "    for image_class in image_classes:\n",
    "        print(\"Processing \", image_class)\n",
    "        items = glob(os.path.join(Images_Path, image_class,\"*\"))\n",
    "        for item in items:\n",
    "            if item.lower().endswith(\".jpg\") or item.lower().endswith(\".bmp\"):\n",
    "                # Read and resize image\n",
    "                full_size_image = cv2.imread(item)\n",
    "                x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "                out = [0] * len(image_classes)\n",
    "                out[i] = 1\n",
    "                y.append(out)\n",
    "        i+=1\n",
    "\n",
    "    print(\"\")\n",
    "    return x,y,image_classes\n",
    "\n",
    "def horizontal_motion_blur(img, blur_factor):\n",
    "    import cv2 \n",
    "    import numpy as np \n",
    "\n",
    "    kernel_size = blur_factor\n",
    "    kernel_h = np.zeros((kernel_size, kernel_size))\n",
    "    kernel_h[int((kernel_size - 1)/2), :] = np.ones(kernel_size) \n",
    "    kernel_h /= kernel_size \n",
    "\n",
    "    # Apply the horizontal kernel. \n",
    "    horizontal_mb = cv2.filter2D(img, -1, kernel_h) \n",
    "    \n",
    "    return horizontal_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad_lighting', 'gblur', 'jp2k', 'refimgs', 'wn', 'mblur']\n",
      "Adding Images: Processing  bad_lighting\n",
      "Processing  gblur\n",
      "Processing  jp2k\n",
      "Processing  refimgs\n",
      "Processing  wn\n",
      "Processing  mblur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ./input/\n",
    "PATH = os.path.abspath(os.path.join('.', 'databaserelease2'))\n",
    "\n",
    "# ./input/sample/images/\n",
    "SOURCE_IMAGES = PATH#os.path.join(PATH, \"sample\", \"images\")\n",
    "x2,y2,image_classes = proc_image_dir(SOURCE_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad_lighting', 'gblur', 'jp2k', 'refimgs', 'wn', 'mblur']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,len(x2)):\n",
    "    if(y2[i][2] > 0):\n",
    "        # Specify the kernel size. \n",
    "        # The greater the size, the more the motion. \n",
    "        \n",
    "        random.seed(221234)\n",
    "        kernel_size_h = random.randrange(30, 60, 5)\n",
    "\n",
    "\n",
    "        # Apply the horizontal kernel. \n",
    "        horizonal_mb = horizontal_motion_blur(x2[i],kernel_size_h)\n",
    "        \n",
    "        x2.append(horizonal_mb)\n",
    "        y2.append([0,0,0,0,0,1])\n",
    "\n",
    "print(image_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 256, 256, 3)\n",
      "(197, 256, 256, 3)\n",
      "(197, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# First split the data in two sets, 60% for training, 40% for Val/Test)\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(x2,y2, test_size=0.4, random_state=1, stratify=y2)\n",
    "\n",
    "# Second split the 40% into validation and test sets\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=1, stratify=y_valtest)\n",
    "\n",
    "print(np.array(X_train).shape)\n",
    "print(np.array(X_val).shape)\n",
    "print(np.array(X_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "K.image_data_format()\n",
    "\n",
    "img_width, img_height = 256, 256\n",
    "nb_train_samples = len(X_train)\n",
    "nb_validation_samples = len(X_val)\n",
    "epochs = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (5, 5), input_shape=(img_width, img_height, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"softmax\"))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"softmax\"))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(128, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"softmax\"))\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(len(image_classes)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 252, 252, 32)      2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 252, 252, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 252, 252, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 124, 124, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                7372864   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 7,469,214\n",
      "Trainable params: 7,468,626\n",
      "Non-trainable params: 588\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "\tloss='binary_crossentropy',\n",
    "\toptimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "\tmetrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 591 samples, validate on 197 samples\n",
      "Epoch 1/50\n",
      "591/591 [==============================] - 17s 28ms/sample - loss: 0.6858 - acc: 0.5615 - val_loss: 0.6697 - val_acc: 0.5110\n",
      "Epoch 2/50\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.6372 - acc: 0.6089 - val_loss: 0.6610 - val_acc: 0.5998\n",
      "Epoch 3/50\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.6065 - acc: 0.6585 - val_loss: 0.6577 - val_acc: 0.5110\n",
      "Epoch 4/50\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.5868 - acc: 0.7030 - val_loss: 0.6384 - val_acc: 0.7090\n",
      "Epoch 5/50\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.5673 - acc: 0.7293 - val_loss: 0.6773 - val_acc: 0.5508\n",
      "Epoch 6/50\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.5512 - acc: 0.7628 - val_loss: 0.6798 - val_acc: 0.5508\n",
      "Epoch 7/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.5309 - acc: 0.7958 - val_loss: 0.6705 - val_acc: 0.5499\n",
      "Epoch 8/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.5141 - acc: 0.8108 - val_loss: 0.6515 - val_acc: 0.6751\n",
      "Epoch 9/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4947 - acc: 0.8373 - val_loss: 0.6417 - val_acc: 0.6751\n",
      "Epoch 10/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4939 - acc: 0.8367 - val_loss: 0.6216 - val_acc: 0.8333\n",
      "Epoch 11/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4831 - acc: 0.8429 - val_loss: 0.6121 - val_acc: 0.8333\n",
      "Epoch 12/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4746 - acc: 0.8519 - val_loss: 0.6150 - val_acc: 0.8350\n",
      "Epoch 13/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4662 - acc: 0.8610 - val_loss: 0.6291 - val_acc: 0.6074\n",
      "Epoch 14/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4600 - acc: 0.8663 - val_loss: 0.6041 - val_acc: 0.8367\n",
      "Epoch 15/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4580 - acc: 0.8582 - val_loss: 0.5721 - val_acc: 0.8350\n",
      "Epoch 16/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4458 - acc: 0.8686 - val_loss: 0.5894 - val_acc: 0.8274\n",
      "Epoch 17/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4390 - acc: 0.8765 - val_loss: 0.5845 - val_acc: 0.7547\n",
      "Epoch 18/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4317 - acc: 0.8849 - val_loss: 0.5644 - val_acc: 0.7826\n",
      "Epoch 19/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4205 - acc: 0.8917 - val_loss: 0.5675 - val_acc: 0.7690\n",
      "Epoch 20/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4166 - acc: 0.8959 - val_loss: 0.5624 - val_acc: 0.7267\n",
      "Epoch 21/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4125 - acc: 0.8900 - val_loss: 0.5492 - val_acc: 0.7800\n",
      "Epoch 22/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.4042 - acc: 0.8996 - val_loss: 0.5477 - val_acc: 0.8359\n",
      "Epoch 23/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3937 - acc: 0.9041 - val_loss: 0.5621 - val_acc: 0.7750\n",
      "Epoch 24/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3933 - acc: 0.9024 - val_loss: 0.5402 - val_acc: 0.8426\n",
      "Epoch 25/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3894 - acc: 0.9038 - val_loss: 0.5088 - val_acc: 0.8909\n",
      "Epoch 26/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3809 - acc: 0.9117 - val_loss: 0.5074 - val_acc: 0.7792\n",
      "Epoch 27/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3716 - acc: 0.9137 - val_loss: 0.4717 - val_acc: 0.8799\n",
      "Epoch 28/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3716 - acc: 0.9168 - val_loss: 0.4830 - val_acc: 0.8553\n",
      "Epoch 29/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3626 - acc: 0.9188 - val_loss: 0.4691 - val_acc: 0.8629\n",
      "Epoch 30/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3554 - acc: 0.9253 - val_loss: 0.5040 - val_acc: 0.8223\n",
      "Epoch 31/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3451 - acc: 0.9340 - val_loss: 0.4270 - val_acc: 0.9069\n",
      "Epoch 32/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3470 - acc: 0.9227 - val_loss: 0.4244 - val_acc: 0.8832\n",
      "Epoch 33/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3430 - acc: 0.9320 - val_loss: 0.4601 - val_acc: 0.8646\n",
      "Epoch 34/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3366 - acc: 0.9337 - val_loss: 0.5604 - val_acc: 0.7107\n",
      "Epoch 35/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3304 - acc: 0.9340 - val_loss: 0.5076 - val_acc: 0.7936\n",
      "Epoch 36/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3326 - acc: 0.9278 - val_loss: 0.5158 - val_acc: 0.7817\n",
      "Epoch 37/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3325 - acc: 0.9270 - val_loss: 0.4438 - val_acc: 0.8409\n",
      "Epoch 38/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3216 - acc: 0.9365 - val_loss: 0.3814 - val_acc: 0.8968\n",
      "Epoch 39/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3123 - acc: 0.9377 - val_loss: 0.3951 - val_acc: 0.9036\n",
      "Epoch 40/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3115 - acc: 0.9405 - val_loss: 0.4013 - val_acc: 0.8917\n",
      "Epoch 41/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3117 - acc: 0.9397 - val_loss: 0.4909 - val_acc: 0.8190\n",
      "Epoch 42/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3061 - acc: 0.9385 - val_loss: 0.5225 - val_acc: 0.7800\n",
      "Epoch 43/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3023 - acc: 0.9425 - val_loss: 0.5030 - val_acc: 0.7453\n",
      "Epoch 44/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2977 - acc: 0.9428 - val_loss: 0.3732 - val_acc: 0.9002\n",
      "Epoch 45/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2949 - acc: 0.9453 - val_loss: 0.4334 - val_acc: 0.8401\n",
      "Epoch 46/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.3013 - acc: 0.9405 - val_loss: 0.3527 - val_acc: 0.9205\n",
      "Epoch 47/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2924 - acc: 0.9413 - val_loss: 0.3747 - val_acc: 0.9002\n",
      "Epoch 48/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2900 - acc: 0.9422 - val_loss: 0.3626 - val_acc: 0.8993\n",
      "Epoch 49/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2804 - acc: 0.9461 - val_loss: 0.3654 - val_acc: 0.9052\n",
      "Epoch 50/50\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2772 - acc: 0.9484 - val_loss: 0.3418 - val_acc: 0.9052\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255, \n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.01,\n",
    "                                   zoom_range=[0.9, 1.25],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=False,\n",
    "                                   fill_mode='reflect')\n",
    "valtest_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow(np.array(X_train), y_train, batch_size=batch_size)\n",
    "validation_generator = valtest_datagen.flow(np.array(X_val), y_val, batch_size=batch_size)\n",
    "test_generator = valtest_datagen.flow(np.array(X_test), y_test, batch_size=batch_size)\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(X_val), np.array(y_val)), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('6040weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('6040weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-793dfc7e03a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'6040weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'blue', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'red', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"./fig2/trvlAcc.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"./fig2/trvlLoss.png\")\n",
    "plt.show()\n",
    "\n",
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "dict_characters = {0: 'Clear', 1: 'Blurry'}\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"./fig2/cm-\"+title+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "a=np.array(X_test)\n",
    "b=np.array(y_test)\n",
    "Y_pred = model.predict(a)\n",
    "\n",
    "\n",
    "Y_pred_classes = np.argmax(Y_pred,axis=1)\n",
    "confusion_mtx = confusion_matrix(np.argmax(b,axis=1), Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = list(image_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 252, 252, 32)      2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 252, 252, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 252, 252, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 124, 124, 64)      18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 124, 124, 64)      256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 124, 124, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 60, 60, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 115200)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                7372864   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 7,469,214\n",
      "Trainable params: 7,468,626\n",
      "Non-trainable params: 588\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "\tloss='binary_crossentropy',\n",
    "\toptimizer=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "\tmetrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 591 samples, validate on 197 samples\n",
      "Epoch 1/2000\n",
      "591/591 [==============================] - 21s 35ms/sample - loss: 0.2729 - acc: 0.9512 - val_loss: 0.3294 - val_acc: 0.9205\n",
      "Epoch 2/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2726 - acc: 0.9501 - val_loss: 0.3232 - val_acc: 0.9306\n",
      "Epoch 3/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2648 - acc: 0.9580 - val_loss: 0.3222 - val_acc: 0.9272\n",
      "Epoch 4/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2612 - acc: 0.9583 - val_loss: 0.3199 - val_acc: 0.9281\n",
      "Epoch 5/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2640 - acc: 0.9571 - val_loss: 0.3177 - val_acc: 0.9298\n",
      "Epoch 6/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2661 - acc: 0.9563 - val_loss: 0.3133 - val_acc: 0.9332\n",
      "Epoch 7/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2582 - acc: 0.9583 - val_loss: 0.3083 - val_acc: 0.9340\n",
      "Epoch 8/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2645 - acc: 0.9602 - val_loss: 0.3100 - val_acc: 0.9289\n",
      "Epoch 9/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2576 - acc: 0.9608 - val_loss: 0.3204 - val_acc: 0.9222\n",
      "Epoch 10/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2623 - acc: 0.9560 - val_loss: 0.3074 - val_acc: 0.9332\n",
      "Epoch 11/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2635 - acc: 0.9566 - val_loss: 0.3133 - val_acc: 0.9222\n",
      "Epoch 12/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2618 - acc: 0.9546 - val_loss: 0.3053 - val_acc: 0.9315\n",
      "Epoch 13/2000\n",
      "591/591 [==============================] - 14s 23ms/sample - loss: 0.2573 - acc: 0.9597 - val_loss: 0.3080 - val_acc: 0.9272\n",
      "Epoch 14/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2551 - acc: 0.9628 - val_loss: 0.3031 - val_acc: 0.9298\n",
      "Epoch 15/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2566 - acc: 0.9616 - val_loss: 0.3065 - val_acc: 0.9255\n",
      "Epoch 16/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2565 - acc: 0.9611 - val_loss: 0.3027 - val_acc: 0.9306\n",
      "Epoch 17/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2648 - acc: 0.9569 - val_loss: 0.3036 - val_acc: 0.9289\n",
      "Epoch 18/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2582 - acc: 0.9614 - val_loss: 0.3086 - val_acc: 0.9281\n",
      "Epoch 19/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2614 - acc: 0.9543 - val_loss: 0.2971 - val_acc: 0.9315\n",
      "Epoch 20/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2557 - acc: 0.9605 - val_loss: 0.2977 - val_acc: 0.9306\n",
      "Epoch 21/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2565 - acc: 0.9616 - val_loss: 0.2958 - val_acc: 0.9323\n",
      "Epoch 22/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2547 - acc: 0.9633 - val_loss: 0.2952 - val_acc: 0.9315\n",
      "Epoch 23/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2550 - acc: 0.9625 - val_loss: 0.2957 - val_acc: 0.9323\n",
      "Epoch 24/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2538 - acc: 0.9594 - val_loss: 0.3039 - val_acc: 0.9298\n",
      "Epoch 25/2000\n",
      "591/591 [==============================] - 15s 26ms/sample - loss: 0.2551 - acc: 0.9577 - val_loss: 0.2992 - val_acc: 0.9323\n",
      "Epoch 26/2000\n",
      "591/591 [==============================] - 16s 27ms/sample - loss: 0.2537 - acc: 0.9608 - val_loss: 0.2953 - val_acc: 0.9298\n",
      "Epoch 27/2000\n",
      "591/591 [==============================] - 16s 26ms/sample - loss: 0.2569 - acc: 0.9597 - val_loss: 0.2971 - val_acc: 0.9272\n",
      "Epoch 28/2000\n",
      "591/591 [==============================] - 16s 26ms/sample - loss: 0.2608 - acc: 0.9535 - val_loss: 0.2972 - val_acc: 0.9281\n",
      "Epoch 29/2000\n",
      "591/591 [==============================] - 16s 26ms/sample - loss: 0.2500 - acc: 0.9619 - val_loss: 0.2986 - val_acc: 0.9255\n",
      "Epoch 30/2000\n",
      "591/591 [==============================] - 17s 29ms/sample - loss: 0.2479 - acc: 0.9639 - val_loss: 0.2976 - val_acc: 0.9264\n",
      "Epoch 31/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2541 - acc: 0.9591 - val_loss: 0.3064 - val_acc: 0.9129\n",
      "Epoch 32/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2522 - acc: 0.9614 - val_loss: 0.2971 - val_acc: 0.9239\n",
      "Epoch 33/2000\n",
      "591/591 [==============================] - 18s 31ms/sample - loss: 0.2511 - acc: 0.9591 - val_loss: 0.2936 - val_acc: 0.9264\n",
      "Epoch 34/2000\n",
      "591/591 [==============================] - 18s 31ms/sample - loss: 0.2559 - acc: 0.9571 - val_loss: 0.2891 - val_acc: 0.9306\n",
      "Epoch 35/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2451 - acc: 0.9659 - val_loss: 0.2912 - val_acc: 0.9289\n",
      "Epoch 36/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2526 - acc: 0.9600 - val_loss: 0.2918 - val_acc: 0.9315\n",
      "Epoch 37/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2501 - acc: 0.9662 - val_loss: 0.3004 - val_acc: 0.9264\n",
      "Epoch 38/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2428 - acc: 0.9656 - val_loss: 0.2957 - val_acc: 0.9264\n",
      "Epoch 39/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2512 - acc: 0.9608 - val_loss: 0.2911 - val_acc: 0.9281\n",
      "Epoch 40/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2514 - acc: 0.9591 - val_loss: 0.2991 - val_acc: 0.9196\n",
      "Epoch 41/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2435 - acc: 0.9647 - val_loss: 0.2913 - val_acc: 0.9272\n",
      "Epoch 42/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2448 - acc: 0.9616 - val_loss: 0.2913 - val_acc: 0.9281\n",
      "Epoch 43/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2449 - acc: 0.9631 - val_loss: 0.2922 - val_acc: 0.9289\n",
      "Epoch 44/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2478 - acc: 0.9614 - val_loss: 0.3006 - val_acc: 0.9196\n",
      "Epoch 45/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2473 - acc: 0.9602 - val_loss: 0.2940 - val_acc: 0.9281\n",
      "Epoch 46/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2400 - acc: 0.9662 - val_loss: 0.2921 - val_acc: 0.9281\n",
      "Epoch 47/2000\n",
      "591/591 [==============================] - 18s 30ms/sample - loss: 0.2470 - acc: 0.9625 - val_loss: 0.2860 - val_acc: 0.9298\n",
      "Epoch 48/2000\n",
      "591/591 [==============================] - 27s 46ms/sample - loss: 0.2414 - acc: 0.9656 - val_loss: 0.2885 - val_acc: 0.9298\n",
      "Epoch 49/2000\n",
      "591/591 [==============================] - 19s 33ms/sample - loss: 0.2385 - acc: 0.9664 - val_loss: 0.2893 - val_acc: 0.9298\n",
      "Epoch 50/2000\n",
      "591/591 [==============================] - 16s 27ms/sample - loss: 0.2420 - acc: 0.9608 - val_loss: 0.2843 - val_acc: 0.9323\n",
      "Epoch 51/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2385 - acc: 0.9642 - val_loss: 0.2856 - val_acc: 0.9332\n",
      "Epoch 52/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2434 - acc: 0.9642 - val_loss: 0.2859 - val_acc: 0.9289\n",
      "Epoch 53/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2407 - acc: 0.9628 - val_loss: 0.2908 - val_acc: 0.9289\n",
      "Epoch 54/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2395 - acc: 0.9676 - val_loss: 0.2878 - val_acc: 0.9323\n",
      "Epoch 55/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2392 - acc: 0.9656 - val_loss: 0.2899 - val_acc: 0.9323\n",
      "Epoch 56/2000\n",
      "591/591 [==============================] - 15s 25ms/sample - loss: 0.2411 - acc: 0.9636 - val_loss: 0.2866 - val_acc: 0.9340\n",
      "Epoch 57/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2361 - acc: 0.9659 - val_loss: 0.2864 - val_acc: 0.9349\n",
      "Epoch 58/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2433 - acc: 0.9605 - val_loss: 0.2895 - val_acc: 0.9281\n",
      "Epoch 59/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2339 - acc: 0.9679 - val_loss: 0.2888 - val_acc: 0.9365\n",
      "Epoch 60/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2470 - acc: 0.9566 - val_loss: 0.2855 - val_acc: 0.9255\n",
      "Epoch 61/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2368 - acc: 0.9659 - val_loss: 0.2952 - val_acc: 0.9230\n",
      "Epoch 62/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2369 - acc: 0.9647 - val_loss: 0.2882 - val_acc: 0.9281\n",
      "Epoch 63/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2384 - acc: 0.9616 - val_loss: 0.2860 - val_acc: 0.9264\n",
      "Epoch 64/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2369 - acc: 0.9656 - val_loss: 0.2836 - val_acc: 0.9306\n",
      "Epoch 65/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2370 - acc: 0.9639 - val_loss: 0.2824 - val_acc: 0.9332\n",
      "Epoch 66/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2370 - acc: 0.9647 - val_loss: 0.2933 - val_acc: 0.9162\n",
      "Epoch 67/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2412 - acc: 0.9633 - val_loss: 0.2848 - val_acc: 0.9323\n",
      "Epoch 68/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2396 - acc: 0.9625 - val_loss: 0.2860 - val_acc: 0.9315\n",
      "Epoch 69/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2345 - acc: 0.9631 - val_loss: 0.2818 - val_acc: 0.9349\n",
      "Epoch 70/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2374 - acc: 0.9636 - val_loss: 0.2811 - val_acc: 0.9306\n",
      "Epoch 71/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2394 - acc: 0.9608 - val_loss: 0.2805 - val_acc: 0.9315\n",
      "Epoch 72/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2307 - acc: 0.9656 - val_loss: 0.3049 - val_acc: 0.9112\n",
      "Epoch 73/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2366 - acc: 0.9642 - val_loss: 0.2817 - val_acc: 0.9332\n",
      "Epoch 74/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2361 - acc: 0.9625 - val_loss: 0.2802 - val_acc: 0.9357\n",
      "Epoch 75/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2288 - acc: 0.9676 - val_loss: 0.2878 - val_acc: 0.9306\n",
      "Epoch 76/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2377 - acc: 0.9636 - val_loss: 0.2942 - val_acc: 0.9213\n",
      "Epoch 77/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2310 - acc: 0.9656 - val_loss: 0.2837 - val_acc: 0.9298\n",
      "Epoch 78/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2340 - acc: 0.9664 - val_loss: 0.2781 - val_acc: 0.9374\n",
      "Epoch 79/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2335 - acc: 0.9636 - val_loss: 0.2795 - val_acc: 0.9306\n",
      "Epoch 80/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2343 - acc: 0.9611 - val_loss: 0.2802 - val_acc: 0.9365\n",
      "Epoch 81/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2345 - acc: 0.9628 - val_loss: 0.2834 - val_acc: 0.9264\n",
      "Epoch 82/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2278 - acc: 0.9670 - val_loss: 0.2816 - val_acc: 0.9272\n",
      "Epoch 83/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2341 - acc: 0.9633 - val_loss: 0.2795 - val_acc: 0.9289\n",
      "Epoch 84/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2237 - acc: 0.9704 - val_loss: 0.2762 - val_acc: 0.9306\n",
      "Epoch 85/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2328 - acc: 0.9639 - val_loss: 0.2750 - val_acc: 0.9264\n",
      "Epoch 86/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2253 - acc: 0.9676 - val_loss: 0.2751 - val_acc: 0.9281\n",
      "Epoch 87/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2280 - acc: 0.9625 - val_loss: 0.2720 - val_acc: 0.9272\n",
      "Epoch 88/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2297 - acc: 0.9619 - val_loss: 0.2813 - val_acc: 0.9239\n",
      "Epoch 89/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2328 - acc: 0.9659 - val_loss: 0.2730 - val_acc: 0.9289\n",
      "Epoch 90/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2280 - acc: 0.9659 - val_loss: 0.2912 - val_acc: 0.9230\n",
      "Epoch 91/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2268 - acc: 0.9662 - val_loss: 0.2778 - val_acc: 0.9281\n",
      "Epoch 92/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2336 - acc: 0.9611 - val_loss: 0.2751 - val_acc: 0.9349\n",
      "Epoch 93/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2265 - acc: 0.9656 - val_loss: 0.2726 - val_acc: 0.9349\n",
      "Epoch 94/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2275 - acc: 0.9662 - val_loss: 0.2813 - val_acc: 0.9255\n",
      "Epoch 95/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2305 - acc: 0.9679 - val_loss: 0.2802 - val_acc: 0.9255\n",
      "Epoch 96/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2277 - acc: 0.9650 - val_loss: 0.2885 - val_acc: 0.9264\n",
      "Epoch 97/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2261 - acc: 0.9645 - val_loss: 0.2810 - val_acc: 0.9281\n",
      "Epoch 98/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2221 - acc: 0.9690 - val_loss: 0.2773 - val_acc: 0.9298\n",
      "Epoch 99/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2246 - acc: 0.9662 - val_loss: 0.2736 - val_acc: 0.9272\n",
      "Epoch 100/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2258 - acc: 0.9681 - val_loss: 0.2755 - val_acc: 0.9247\n",
      "Epoch 101/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2256 - acc: 0.9670 - val_loss: 0.2745 - val_acc: 0.9289\n",
      "Epoch 102/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2249 - acc: 0.9681 - val_loss: 0.2736 - val_acc: 0.9255\n",
      "Epoch 103/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2216 - acc: 0.9673 - val_loss: 0.2732 - val_acc: 0.9239\n",
      "Epoch 104/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2263 - acc: 0.9645 - val_loss: 0.2731 - val_acc: 0.9281\n",
      "Epoch 105/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2174 - acc: 0.9698 - val_loss: 0.2809 - val_acc: 0.9239\n",
      "Epoch 106/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2189 - acc: 0.9718 - val_loss: 0.2720 - val_acc: 0.9298\n",
      "Epoch 107/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2253 - acc: 0.9679 - val_loss: 0.2897 - val_acc: 0.9137\n",
      "Epoch 108/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2231 - acc: 0.9670 - val_loss: 0.2727 - val_acc: 0.9272\n",
      "Epoch 109/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2238 - acc: 0.9667 - val_loss: 0.2694 - val_acc: 0.9365\n",
      "Epoch 110/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2197 - acc: 0.9664 - val_loss: 0.2674 - val_acc: 0.9357\n",
      "Epoch 111/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2244 - acc: 0.9642 - val_loss: 0.2774 - val_acc: 0.9205\n",
      "Epoch 112/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2206 - acc: 0.9684 - val_loss: 0.2723 - val_acc: 0.9315\n",
      "Epoch 113/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2231 - acc: 0.9681 - val_loss: 0.2699 - val_acc: 0.9323\n",
      "Epoch 114/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2251 - acc: 0.9628 - val_loss: 0.2725 - val_acc: 0.9332\n",
      "Epoch 115/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2229 - acc: 0.9645 - val_loss: 0.2753 - val_acc: 0.9281\n",
      "Epoch 116/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2180 - acc: 0.9684 - val_loss: 0.2706 - val_acc: 0.9332\n",
      "Epoch 117/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2193 - acc: 0.9662 - val_loss: 0.2749 - val_acc: 0.9281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2166 - acc: 0.9695 - val_loss: 0.2768 - val_acc: 0.9179\n",
      "Epoch 119/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2213 - acc: 0.9642 - val_loss: 0.2687 - val_acc: 0.9239\n",
      "Epoch 120/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2178 - acc: 0.9681 - val_loss: 0.2737 - val_acc: 0.9247\n",
      "Epoch 121/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2227 - acc: 0.9670 - val_loss: 0.2721 - val_acc: 0.9264\n",
      "Epoch 122/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2154 - acc: 0.9698 - val_loss: 0.2704 - val_acc: 0.9306\n",
      "Epoch 123/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2226 - acc: 0.9628 - val_loss: 0.2669 - val_acc: 0.9306\n",
      "Epoch 124/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2187 - acc: 0.9670 - val_loss: 0.2632 - val_acc: 0.9298\n",
      "Epoch 125/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2204 - acc: 0.9645 - val_loss: 0.2730 - val_acc: 0.9264\n",
      "Epoch 126/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2229 - acc: 0.9628 - val_loss: 0.2718 - val_acc: 0.9230\n",
      "Epoch 127/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2197 - acc: 0.9670 - val_loss: 0.2666 - val_acc: 0.9315\n",
      "Epoch 128/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2197 - acc: 0.9653 - val_loss: 0.2705 - val_acc: 0.9315\n",
      "Epoch 129/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2136 - acc: 0.9662 - val_loss: 0.2701 - val_acc: 0.9264\n",
      "Epoch 130/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2125 - acc: 0.9659 - val_loss: 0.2689 - val_acc: 0.9315\n",
      "Epoch 131/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2144 - acc: 0.9679 - val_loss: 0.2673 - val_acc: 0.9315\n",
      "Epoch 132/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2171 - acc: 0.9667 - val_loss: 0.2645 - val_acc: 0.9357\n",
      "Epoch 133/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2149 - acc: 0.9662 - val_loss: 0.2617 - val_acc: 0.9349\n",
      "Epoch 134/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2149 - acc: 0.9670 - val_loss: 0.2613 - val_acc: 0.9315\n",
      "Epoch 135/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2117 - acc: 0.9704 - val_loss: 0.2661 - val_acc: 0.9264\n",
      "Epoch 136/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2133 - acc: 0.9701 - val_loss: 0.2809 - val_acc: 0.9188\n",
      "Epoch 137/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2115 - acc: 0.9681 - val_loss: 0.2654 - val_acc: 0.9332\n",
      "Epoch 138/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2172 - acc: 0.9650 - val_loss: 0.2704 - val_acc: 0.9272\n",
      "Epoch 139/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2138 - acc: 0.9693 - val_loss: 0.2677 - val_acc: 0.9340\n",
      "Epoch 140/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2074 - acc: 0.9721 - val_loss: 0.2693 - val_acc: 0.9323\n",
      "Epoch 141/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2132 - acc: 0.9659 - val_loss: 0.2645 - val_acc: 0.9323\n",
      "Epoch 142/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2131 - acc: 0.9670 - val_loss: 0.2652 - val_acc: 0.9340\n",
      "Epoch 143/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2107 - acc: 0.9698 - val_loss: 0.2618 - val_acc: 0.9272\n",
      "Epoch 144/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2108 - acc: 0.9701 - val_loss: 0.2618 - val_acc: 0.9332\n",
      "Epoch 145/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2070 - acc: 0.9712 - val_loss: 0.2572 - val_acc: 0.9349\n",
      "Epoch 146/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2096 - acc: 0.9710 - val_loss: 0.2608 - val_acc: 0.9315\n",
      "Epoch 147/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2078 - acc: 0.9724 - val_loss: 0.2633 - val_acc: 0.9365\n",
      "Epoch 148/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2075 - acc: 0.9701 - val_loss: 0.2615 - val_acc: 0.9315\n",
      "Epoch 149/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2087 - acc: 0.9687 - val_loss: 0.2619 - val_acc: 0.9289\n",
      "Epoch 150/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2094 - acc: 0.9684 - val_loss: 0.2715 - val_acc: 0.9247\n",
      "Epoch 151/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2172 - acc: 0.9628 - val_loss: 0.2634 - val_acc: 0.9255\n",
      "Epoch 152/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2085 - acc: 0.9676 - val_loss: 0.2571 - val_acc: 0.9382\n",
      "Epoch 153/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2046 - acc: 0.9693 - val_loss: 0.2595 - val_acc: 0.9374\n",
      "Epoch 154/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2064 - acc: 0.9690 - val_loss: 0.2595 - val_acc: 0.9298\n",
      "Epoch 155/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2102 - acc: 0.9670 - val_loss: 0.2584 - val_acc: 0.9340\n",
      "Epoch 156/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2102 - acc: 0.9673 - val_loss: 0.2647 - val_acc: 0.9264\n",
      "Epoch 157/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2107 - acc: 0.9695 - val_loss: 0.2605 - val_acc: 0.9264\n",
      "Epoch 158/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2052 - acc: 0.9695 - val_loss: 0.2595 - val_acc: 0.9332\n",
      "Epoch 159/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2039 - acc: 0.9710 - val_loss: 0.2569 - val_acc: 0.9365\n",
      "Epoch 160/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2115 - acc: 0.9645 - val_loss: 0.2636 - val_acc: 0.9323\n",
      "Epoch 161/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2062 - acc: 0.9707 - val_loss: 0.2598 - val_acc: 0.9374\n",
      "Epoch 162/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2085 - acc: 0.9664 - val_loss: 0.2579 - val_acc: 0.9391\n",
      "Epoch 163/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2090 - acc: 0.9650 - val_loss: 0.2552 - val_acc: 0.9340\n",
      "Epoch 164/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2073 - acc: 0.9690 - val_loss: 0.2588 - val_acc: 0.9357\n",
      "Epoch 165/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2093 - acc: 0.9673 - val_loss: 0.2574 - val_acc: 0.9365\n",
      "Epoch 166/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2061 - acc: 0.9679 - val_loss: 0.2576 - val_acc: 0.9450\n",
      "Epoch 167/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2024 - acc: 0.9704 - val_loss: 0.2596 - val_acc: 0.9365\n",
      "Epoch 168/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2015 - acc: 0.9715 - val_loss: 0.2573 - val_acc: 0.9365\n",
      "Epoch 169/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2030 - acc: 0.9701 - val_loss: 0.2598 - val_acc: 0.9332\n",
      "Epoch 170/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2057 - acc: 0.9673 - val_loss: 0.2655 - val_acc: 0.9272\n",
      "Epoch 171/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2001 - acc: 0.9715 - val_loss: 0.2609 - val_acc: 0.9298\n",
      "Epoch 172/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2013 - acc: 0.9715 - val_loss: 0.2549 - val_acc: 0.9332\n",
      "Epoch 173/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2037 - acc: 0.9715 - val_loss: 0.2555 - val_acc: 0.9332\n",
      "Epoch 174/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2010 - acc: 0.9698 - val_loss: 0.2654 - val_acc: 0.9264\n",
      "Epoch 175/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2066 - acc: 0.9679 - val_loss: 0.2648 - val_acc: 0.9306\n",
      "Epoch 176/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2005 - acc: 0.9704 - val_loss: 0.2653 - val_acc: 0.9315\n",
      "Epoch 177/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2021 - acc: 0.9673 - val_loss: 0.2541 - val_acc: 0.9357\n",
      "Epoch 178/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2024 - acc: 0.9687 - val_loss: 0.2546 - val_acc: 0.9365\n",
      "Epoch 179/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2078 - acc: 0.9684 - val_loss: 0.2607 - val_acc: 0.9323\n",
      "Epoch 180/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2003 - acc: 0.9693 - val_loss: 0.2581 - val_acc: 0.9315\n",
      "Epoch 181/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2004 - acc: 0.9695 - val_loss: 0.2709 - val_acc: 0.9196\n",
      "Epoch 182/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2062 - acc: 0.9659 - val_loss: 0.2570 - val_acc: 0.9365\n",
      "Epoch 183/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2033 - acc: 0.9707 - val_loss: 0.2780 - val_acc: 0.9146\n",
      "Epoch 184/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1952 - acc: 0.9746 - val_loss: 0.2552 - val_acc: 0.9349\n",
      "Epoch 185/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1966 - acc: 0.9707 - val_loss: 0.2614 - val_acc: 0.9255\n",
      "Epoch 186/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2007 - acc: 0.9690 - val_loss: 0.2598 - val_acc: 0.9272\n",
      "Epoch 187/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1978 - acc: 0.9690 - val_loss: 0.2564 - val_acc: 0.9349\n",
      "Epoch 188/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2051 - acc: 0.9656 - val_loss: 0.2567 - val_acc: 0.9391\n",
      "Epoch 189/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2058 - acc: 0.9687 - val_loss: 0.2539 - val_acc: 0.9425\n",
      "Epoch 190/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2014 - acc: 0.9681 - val_loss: 0.2715 - val_acc: 0.9188\n",
      "Epoch 191/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2000 - acc: 0.9690 - val_loss: 0.2574 - val_acc: 0.9340\n",
      "Epoch 192/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2021 - acc: 0.9693 - val_loss: 0.2563 - val_acc: 0.9357\n",
      "Epoch 193/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1994 - acc: 0.9684 - val_loss: 0.2574 - val_acc: 0.9306\n",
      "Epoch 194/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1970 - acc: 0.9690 - val_loss: 0.2697 - val_acc: 0.9188\n",
      "Epoch 195/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2015 - acc: 0.9667 - val_loss: 0.2510 - val_acc: 0.9391\n",
      "Epoch 196/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2017 - acc: 0.9670 - val_loss: 0.2570 - val_acc: 0.9340\n",
      "Epoch 197/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1962 - acc: 0.9695 - val_loss: 0.2552 - val_acc: 0.9357\n",
      "Epoch 198/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1937 - acc: 0.9735 - val_loss: 0.2507 - val_acc: 0.9315\n",
      "Epoch 199/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.2005 - acc: 0.9676 - val_loss: 0.2588 - val_acc: 0.9289\n",
      "Epoch 200/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1973 - acc: 0.9707 - val_loss: 0.2561 - val_acc: 0.9306\n",
      "Epoch 201/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1921 - acc: 0.9726 - val_loss: 0.2546 - val_acc: 0.9365\n",
      "Epoch 202/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1939 - acc: 0.9701 - val_loss: 0.2561 - val_acc: 0.9340\n",
      "Epoch 203/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1985 - acc: 0.9670 - val_loss: 0.2495 - val_acc: 0.9349\n",
      "Epoch 204/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1912 - acc: 0.9710 - val_loss: 0.2508 - val_acc: 0.9416\n",
      "Epoch 205/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1982 - acc: 0.9676 - val_loss: 0.2640 - val_acc: 0.9255\n",
      "Epoch 206/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1947 - acc: 0.9701 - val_loss: 0.2547 - val_acc: 0.9315\n",
      "Epoch 207/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1981 - acc: 0.9690 - val_loss: 0.2500 - val_acc: 0.9332\n",
      "Epoch 208/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1934 - acc: 0.9695 - val_loss: 0.2565 - val_acc: 0.9340\n",
      "Epoch 209/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1948 - acc: 0.9687 - val_loss: 0.2492 - val_acc: 0.9374\n",
      "Epoch 210/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1885 - acc: 0.9755 - val_loss: 0.2478 - val_acc: 0.9442\n",
      "Epoch 211/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1952 - acc: 0.9704 - val_loss: 0.2476 - val_acc: 0.9323\n",
      "Epoch 212/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1915 - acc: 0.9724 - val_loss: 0.2452 - val_acc: 0.9391\n",
      "Epoch 213/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1934 - acc: 0.9724 - val_loss: 0.2477 - val_acc: 0.9365\n",
      "Epoch 214/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1918 - acc: 0.9710 - val_loss: 0.2543 - val_acc: 0.9349\n",
      "Epoch 215/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1943 - acc: 0.9687 - val_loss: 0.2486 - val_acc: 0.9306\n",
      "Epoch 216/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1940 - acc: 0.9690 - val_loss: 0.2585 - val_acc: 0.9281\n",
      "Epoch 217/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1929 - acc: 0.9693 - val_loss: 0.2590 - val_acc: 0.9332\n",
      "Epoch 218/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1899 - acc: 0.9690 - val_loss: 0.2522 - val_acc: 0.9340\n",
      "Epoch 219/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1979 - acc: 0.9659 - val_loss: 0.2582 - val_acc: 0.9306\n",
      "Epoch 220/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1864 - acc: 0.9710 - val_loss: 0.2561 - val_acc: 0.9323\n",
      "Epoch 221/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1885 - acc: 0.9738 - val_loss: 0.2472 - val_acc: 0.9306\n",
      "Epoch 222/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1906 - acc: 0.9707 - val_loss: 0.2466 - val_acc: 0.9323\n",
      "Epoch 223/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1924 - acc: 0.9707 - val_loss: 0.2511 - val_acc: 0.9365\n",
      "Epoch 224/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1931 - acc: 0.9679 - val_loss: 0.2480 - val_acc: 0.9374\n",
      "Epoch 225/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1882 - acc: 0.9726 - val_loss: 0.2532 - val_acc: 0.9374\n",
      "Epoch 226/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1901 - acc: 0.9695 - val_loss: 0.2486 - val_acc: 0.9349\n",
      "Epoch 227/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1863 - acc: 0.9718 - val_loss: 0.2494 - val_acc: 0.9332\n",
      "Epoch 228/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1888 - acc: 0.9707 - val_loss: 0.2484 - val_acc: 0.9382\n",
      "Epoch 229/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1884 - acc: 0.9712 - val_loss: 0.2510 - val_acc: 0.9357\n",
      "Epoch 230/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1923 - acc: 0.9698 - val_loss: 0.2486 - val_acc: 0.9382\n",
      "Epoch 231/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1892 - acc: 0.9701 - val_loss: 0.2576 - val_acc: 0.9382\n",
      "Epoch 232/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1895 - acc: 0.9687 - val_loss: 0.2629 - val_acc: 0.9230\n",
      "Epoch 233/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1836 - acc: 0.9746 - val_loss: 0.2514 - val_acc: 0.9281\n",
      "Epoch 234/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1870 - acc: 0.9698 - val_loss: 0.2503 - val_acc: 0.9323\n",
      "Epoch 235/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1870 - acc: 0.9681 - val_loss: 0.2546 - val_acc: 0.9272\n",
      "Epoch 236/2000\n",
      "591/591 [==============================] - 17s 29ms/sample - loss: 0.1824 - acc: 0.9738 - val_loss: 0.2521 - val_acc: 0.9365\n",
      "Epoch 237/2000\n",
      "591/591 [==============================] - 15s 26ms/sample - loss: 0.1845 - acc: 0.9707 - val_loss: 0.2521 - val_acc: 0.9323\n",
      "Epoch 238/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1878 - acc: 0.9687 - val_loss: 0.2494 - val_acc: 0.9340\n",
      "Epoch 239/2000\n",
      "591/591 [==============================] - 14s 24ms/sample - loss: 0.1834 - acc: 0.9710 - val_loss: 0.2522 - val_acc: 0.9289\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255, \n",
    "                                   rotation_range=15,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   shear_range=0.01,\n",
    "                                   zoom_range=[0.90, 1.1],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=False,\n",
    "                                   fill_mode='reflect')\n",
    "valtest_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow(np.array(X_train), y_train, batch_size=batch_size)\n",
    "validation_generator = valtest_datagen.flow(np.array(X_val), y_val, batch_size=batch_size)\n",
    "test_generator = valtest_datagen.flow(np.array(X_test), y_test, batch_size=batch_size)\n",
    "cb = tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=27,verbose=0,mode='auto',baseline=None,restore_best_weights=True)\n",
    "\n",
    "history2 = model.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(X_val), np.array(y_val)), epochs=2000, callbacks=[cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('6040weights2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history2.history['acc']\n",
    "val_acc = history2.history['val_acc']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'blue', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'red', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"./fig2/trvlAcc2.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'blue', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'red', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"./fig2/trvlLoss2.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('6040weights2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "dict_characters = {0: 'Clear', 1: 'Blurry'}\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(\"./fig2/cm2-\"+title+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    \n",
    "a=np.array(X_test).astype(float)\n",
    "b=np.array(y_test)\n",
    "Y_pred = model.predict(a)\n",
    "\n",
    "\n",
    "Y_pred_classes = np.argmax(Y_pred,axis=1)\n",
    "confusion_mtx = confusion_matrix(np.argmax(b,axis=1), Y_pred_classes) \n",
    "plot_confusion_matrix(confusion_mtx, classes = list(image_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8223350253807107"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(Y_pred_classes,np.argmax(b,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  0,  0,  1,  0,  0],\n",
       "       [ 0, 33,  7,  0,  0,  2],\n",
       "       [ 0,  4, 37,  3,  1,  0],\n",
       "       [ 1,  0,  4, 17,  1,  3],\n",
       "       [ 0,  0,  8,  0, 27,  0],\n",
       "       [ 0,  0,  0,  0,  0, 45]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 42, 45, 26, 35, 45]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_totals = [sum(i) for i in confusion_mtx]\n",
    "class_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 33, 37, 17, 27, 45]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_correct_pred = [confusion_mtx[i][i] for i in range(6)]\n",
    "class_correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75,\n",
       " 0.7857142857142857,\n",
       " 0.8222222222222222,\n",
       " 0.6538461538461539,\n",
       " 0.7714285714285715,\n",
       " 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = [class_correct_pred[i]/class_totals[i] for i in range(6)]\n",
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.75,\n",
       " 0.7857142857142857,\n",
       " 0.8222222222222222,\n",
       " 0.6538461538461539,\n",
       " 0.7714285714285715,\n",
       " 1.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_totals = [sum(i) for i in confusion_mtx]\n",
    "class_totals\n",
    "\n",
    "tp = [confusion_mtx[i][i] for i in range(6)]\n",
    "tp\n",
    "\n",
    "sensetivity = [tp[i]/class_totals[i] for i in range(6)]\n",
    "sensetivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 19, 4, 2, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = [[sum(x) for x in zip(*confusion_mtx)][i] - tp[i] for i in range(6)]\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9948453608247423,\n",
       " 0.975609756097561,\n",
       " 0.88125,\n",
       " 0.9777777777777777,\n",
       " 0.9882352941176471,\n",
       " 0.9671052631578947]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumall = sum(map(sum, confusion_mtx))\n",
    "print(sumall)\n",
    "tn = [sumall - [sum(x) for x in zip(*confusion_mtx)][i] for i in range(6)]\n",
    "tn\n",
    "specificity = [tn[i]/(tn[i]+fp[i]) for i in range(6)]\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
